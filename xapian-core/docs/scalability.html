<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<HTML>
<HEAD>
<TITLE>Xapian: Scalability</TITLE>
</HEAD>
<BODY BGCOLOR="white" TEXT="black">

<H1>Scalability</H1>

<P>People often want to know how Xapian will scale.  The short answer is "very
well" - a previous version of the software powered BrightStation's Webtop
search engine, which offered a search over around 500 million web pages (around
1.5 terabytes of database files).  Searches took less than a second.

<H2>General Scalability Considerations</H2>

<P>In a large search application, I/O will end up being
the limiting factor.  So you want a RAID setup optimised for fast reading,
lots of RAM in the box so the OS can cache lots of disk blocks (the access
patterns typically mean that you only need to cache a few percent of the
database to eliminate most disk cache misses).

<P>It also means that reducing the database size is usually a win.  Quartz
compresses the information in the tables in ways which work well given the
nature of the data but aren't too expensive to unpack (e.g. lists of sorted
docids are stored as differences with smaller values encoded in fewer bytes).
There is further potential for improving the encodings used.

<P>Another way to reduce disk I/O is to run databases through quartzcompact.
Quartz usually leaves some spare space in each block so that updates are more
efficient - quartzcompact makes all blocks as full as possible, and so reduces
the size of the database.  It also produces a database with revision 1 which is
inherently faster to search.  The penalty is that updates will be slow for a
while, as they'll result in a lot of block splitting when all blocks are full.

<P>In general, I'd suggest a strategy of splitting the data over several
databases.  Once each has finished being updated, run it through quartzcompact
to make it small and faster to search.

<P>A multiple-database scheme works particularly well if you want a rolling web
index where the contents of the oldest database can be rechecked and live links
put back into a new database which, once built, replaces the oldest database.
It's also good for a news-type application where older documents should expire
from the index.

<P>You could take this idea further by implementing an ultra-compact read-only
backend which would take a quartz b-tree and convert it to something like a cdb
hashed file.  The same information would be stored, but with less overhead than
quartz b-trees (which need to allow for update).  If you need serious
performance, implementing such a backend is worth considering.

<H2>Size Limits in Xapian</H2>

<P>The quartz database backend (which is the backend you'd usually use) stores
the indexes in several files containing B-tree tables.  If you're indexing with
positional information (for phrase searching) the term positions table is
usually the largest.

<P>
The current limits are:

<ul>
<li>
  Xapian uses unsigned 32-bit ints for document ids, so you're limited to
  just over 4 billion documents in a database (as of 2003-09-10 that's
  more than a third larger than Google).  The other limits will cut in first
  for a single database, but searches over multiple databases are done by
  interleaving the document ids, so this might start to matter (especially if
  one database is much larger than the others).  This interleaving
  technique could be changed fairly easily if it proves problematic.

<li>
  If you search many databases concurrently, you may hit the per-process
  file-descriptor limit - each quartz database uses 5 fds.  Some Unix-like OSes
  allow this limit to be raised.  Another way to avoid it (and to spread the
  search load) is to use the remote backend to search databases on a cluster of
  machines.  Quartz could be made to not open fds for tables which aren't being
  used during search (values and positions may not be), or to juggle fds - the
  record table is typically only used for results, while the posting table is
  typically only used during matching.

<li>
  If the OS has a filesize limit, that obviously applies to Xapian (a 2GB limit
  is common for older operating systems).  The old Muscat36 system supported
  splitting the B-tree over multiple 1GB files, but native support for larger
  files is available in any serious modern OS so quartz doesn't support this
  (something similar could be implemented, but it no longer seems necessary).
  <P>
  So what is the limit for a modern OS?  Taking Linux 2.4 with the ext2
  filing system as an example, quoting from
  linux/Documentation/filesystems/ext2.txt:
  <blockquote><pre>
  Filesystem block size:     1kB        2kB        4kB        8kB

  File size limit:          16GB      256GB     2048GB     2048GB
  Filesystem size limit:  2047GB     8192GB    16384GB    32768GB

  There is a 2.4 kernel limit of 2048GB for a single block device, so no
  filesystem larger than that can be created at this time.  There is also
  an upper limit on the block size imposed by the page size of the kernel,
  so 8kB blocks are only allowed on Alpha systems (and other architectures
  which support larger pages).
  </pre></blockquote>

<li>
  The B-trees use a 32-bit unsigned block count.  The default blocksize is 8K
  which limits you to 32TB tables.  You can increase the blocksize if this
  is a problem, but it's best to do it before you create the database as
  otherwise you need to use copydatabase to rebuild the database with the
  new blocksize, and that will take a while for such a large database.
  The maximum blocksize quartz allows is 64K, which limits you to 256TB
  tables.

<li>
  Quartz stores the total length (i.e. number of terms) of all the documents in
  a database so it can calculate the average document length.  This is
  currently stored as an unsigned 64-bit quantity so it's not likely to be a
  limit you'll hit.  I've listed it for completeness.
</ul>

If you've further questions about scalability, ask on the mailing lists - people
using Xapian to search large databases may be able to make further suggestions.

<!-- FOOTER $Author$ $Date$ $Id$ -->
</BODY>
</HTML>
